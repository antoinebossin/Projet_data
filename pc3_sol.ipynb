{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC 3 : Réduction de dimension - 16 juin 2025 - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook, nous allons effectuer une analyse en composantes principales d'un jeu de données décrivant les scores obtenus par les meilleurs athlètes ayant participé en 2004 à une épreuve de décathlon, aux Jeux Olympiques d'Athènes ou au Décastar de Talence.\n",
    "\n",
    "Nous utiliserons pour cela la toolbox [`scikit-learn`](http://scikit-learn.org/stable/index.html). `scikit-learn` est un ensemble de librairies de machine learning très utilisée et qui implémente la plupart des algorithmes d'apprentissage automatique non-profond. Il s'agit d'un projet Open Source. \n",
    "\n",
    "Dans une deuxième partie, vous vous essayerez à implémenter l'ACP vous-mêmes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies python \n",
    "\n",
    "Ce notebook a été créé avec les versions suivantes des librairies :\n",
    "* numpy 2.2.4\n",
    "* matplotlib 3.10.1\n",
    "* pandas 2.2.3\n",
    "* sklearn 1.6.1\n",
    "* seaborn 0.13.2\n",
    "\n",
    "Tout devrait bien se passer si vous êtes bien dans l'environnement conda `sdd2025` chargé grâce au fichier `environment.yml` à la racine du répertoire. \n",
    "\n",
    "Des différences de version peuvent expliquer des comportements inattendus (avertissements, messages d'erreurs, fonctionalités inexistantes) mais il n'est pas nécessaire a priori d'avoir exactement les versions listées ci-dessus. Si vous souhaitez vérifier les versions des différentes librairies que vous utilisez, vous pouvez utiliser le code suivant :\n",
    "```python\n",
    "import <nom du module>\n",
    "print(<nom du module>.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import de numpy et matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', **{'size': 12}) # règle la taille de police globalement pour les plots (en pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données sont contenues dans le fichier `decathlon.txt`.\n",
    "\n",
    "Le fichier contient 42 lignes et 13 colonnes.\n",
    "\n",
    "La première ligne est un en-tête qui décrit les contenus des colonnes.\n",
    "\n",
    "Les lignes suivantes décrivent les 41 athlètes.\n",
    "\n",
    "Les 10 premières colonnes contiennent les scores obtenus aux différentes épreuves.\n",
    "La 11ème colonne contient le classement.\n",
    "La 12ème colonne contient le nombre de points obtenus.\n",
    "La 13ème colonne contient une variable qualitative qui précise l'épreuve (JO ou Décastar) concernée.\n",
    "\n",
    "Nous allons examiner ces données avec la librairie `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/decathlon.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m my_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/decathlon.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# lire les données dans un dataframe\u001b[39;00m\n",
      "File \u001b[1;32mC:\\miniconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\miniconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\miniconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\miniconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\miniconda\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/decathlon.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "my_data = pd.read_csv('data/decathlon.txt', sep=\"\\t\")  # lire les données dans un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quelques rappels sur `pandas`\n",
    "\n",
    "Assurez-vous que vous comprenez l'usage des fonctionalités ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Lister les noms des colonnes\n",
    "my_columns = list(my_data.columns) \n",
    "print(my_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner une colonne\n",
    "my_data['100m'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner plusieurs colonnes\n",
    "columns = ['100m', '400m']\n",
    "my_data[columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner des lignes à partir de la valeur d'une colonne\n",
    "my_data[my_data['Competition']=='OlympicG'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Dimensions des données\n",
    "print(my_data.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeurs du tableau sous forme d'array numpy\n",
    "my_data_values = my_data.values\n",
    "print(type(my_data_values))\n",
    "print(my_data_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Type des colonnes\n",
    "print(my_data.dtypes)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre d'occurrences de chaque valeur unique dans une colonne\n",
    "my_data['Competition'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Distributions des variables\n",
    "Visualisons la distribution des 10 variables continues représentant les performances aux 10 épreuves (c'est à dire les 10 premières variables) grâce à des histogrammes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Histogrammes pour les variables continues\n",
    "for (feat_idx, feat_name) in enumerate(my_data.columns[:10]):\n",
    "    # créer une sous-figure (subplot) à la position (feat_idx+1) d'une grille 2x5\n",
    "    ax = fig.add_subplot(2, 5, (feat_idx+1))\n",
    "\n",
    "    # afficher l'histogramme de la variable feat_name\n",
    "    h = ax.hist(my_data[feat_name], bins=10, edgecolor='none')\n",
    "\n",
    "    # utiliser le nom de la variable comme titre pour chaque histogramme\n",
    "    ax.set_title(feat_name)\n",
    "    \n",
    "# espacement entre les subplots\n",
    "fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons aussi visualiser la distribution de ces variables en fonction de l'épreuve :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Histogrammes pour les variables continues\n",
    "for (feat_idx, feat_name) in enumerate(my_data.columns[:10]):\n",
    "    # créer une sous-figure (subplot) à la position (feat_idx+1) d'une grille 2x5\n",
    "    ax = fig.add_subplot(2, 5, (feat_idx+1))\n",
    "\n",
    "    # afficher l'histogramme de la variable feat_name pour Competition=OlympicG\n",
    "    h = ax.hist(my_data[my_data['Competition']=='OlympicG'][feat_name], bins=10,  \n",
    "                color='tab:blue', edgecolor='none', alpha=1, label='Olympic Games')\n",
    "    \n",
    "    # afficher l'histogramme de la variable feat_name pour Competition=Decastar\n",
    "    h = ax.hist(my_data[my_data['Competition']=='Decastar'][feat_name], bins=10,  \n",
    "                color='tab:orange', edgecolor='none', alpha=0.8, label='Decastar')\n",
    "\n",
    "    # utiliser le nom de la variable comme titre pour chaque histogramme\n",
    "    ax.set_title(feat_name)\n",
    "\n",
    "# Légende\n",
    "plt.legend()\n",
    "\n",
    "# espacement entre les subplots\n",
    "fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Les deux compétitions vous semblent-elles avoir des différences ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__ Les distributions sont sensiblement les mêmes ; il y a plus d'entrées pour les JOs que pour le Decastar, on voit donc plus de valeurs « extrêmes » pour les JOs (l'histogramme bleu est un peu plus « étalé » que l'histogramme orange)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Couples de variables\n",
    "Représentons les différents athlètes selon leur performance au 400m et au lancer du poids (`Shot.put`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5)) # forcer une figure carrée\n",
    "\n",
    "plt.scatter(my_data['400m'], my_data['Shot.put'], s=50)\n",
    "\n",
    "plt.xlabel('Performance au 400m')\n",
    "plt.ylabel('Performance au lancer du poids')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fourchettes de valeur prises par ces deux variables sont différentes, leurs unités aussi. Pour mieux les comparer il est souhaitable de les standardiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5)) # forcer une figure carrée\n",
    "\n",
    "data_400m_std = (my_data['400m'] - np.mean(my_data['400m']))/(np.std(my_data['400m']))\n",
    "data_shotput_std = (my_data['Shot.put'] - np.mean(my_data['Shot.put']))/(np.std(my_data['Shot.put']))\n",
    "\n",
    "plt.scatter(data_400m_std, data_shotput_std, s=50)\n",
    "\n",
    "plt.xlabel('400m (performance standardisée)')\n",
    "plt.ylabel('Lancer du poids (performance standardisée)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Ce nuage de points vous semble-t-il suggérer que les deux performances sont corrélées ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__ Non (cf. les exemples à la fin du Chapitre 2 du poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative avec seaborn\n",
    "La librairie `seaborn` permet des visualisations plus élaborées que `matplotlib`. Vous pouvez par exemple explorer les capacités de [jointplot](https://seaborn.pydata.org/generated/seaborn.jointplot.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# sns.set_style('whitegrid')\n",
    "\n",
    "sns.jointplot(x='400m', y='Shot.put', data=my_data, \n",
    "              height=5, space=0, color='tab:blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corrélation de deux variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons calculer leur coefficient de corrélation de Pearson grâce au module `scipy.stats`. La fonction `pearsonr` renvoie même une p-valeur : https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, pval = st.pearsonr(my_data['400m'], my_data['Shot.put'])\n",
    "print(f\"Corrélation entre le lancer du poids et le 400m : {r:.2f} (p={pval:.2e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Que signifie la valeur entre parenthèses précédée de `p=` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__ Il s'agit de la p-valeur d'un test statistique dont l'hypothèse nulle est l'absence de corrélation entre les deux variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Cela vous semble-t-il en accord avec le nuage de points ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__ Oui. L'absence de corrélation est confirmée par la p-valeur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Choisissez deux variables dont il vous semble vraisemblable qu'elles soient corrélées. Affichez le nuage de points correspondant. Calculez le coefficient de corrélation de Pearson. Souhaitez-vous rejeter votre hypothèse ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse (exemple) :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, pval = st.pearsonr(my_data['100m'], my_data['110m.hurdle'])\n",
    "print(f\"Corrélation entre le 100m et le 110m haies : {r:.2f} (p={pval:.2e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 4)) # forcer une figure carrée\n",
    "\n",
    "data_100m_std = (my_data['100m'] - np.mean(my_data['100m']))/(np.std(my_data['100m']))\n",
    "data_hurdles_std = (my_data['110m.hurdle'] - np.mean(my_data['110m.hurdle']))/(np.std(my_data['110m.hurdle']))\n",
    "\n",
    "\n",
    "plt.scatter(data_100m_std, data_hurdles_std, s=50)\n",
    "\n",
    "plt.xlabel('100m (performance standardisée)')\n",
    "plt.ylabel('110m haies (performance standardisée)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sans surprise, les performances au 110m haies et au 100 m sont corrélées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrice de corrélation\n",
    "\n",
    "Plutôt que de regarder les variables deux par deux, nous pouvons avoir une vue d'ensemble de leurs corrélations en affichant leur matrice de corrélation : \n",
    "* la méthode [DataFrame.corr](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) de `pandas` nous permet de calculer cette matrice ;\n",
    "* la méthode [heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html) de `seaborn` nous permet de la visualiser par une carte thermique (_heatmap_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Calcul de la matrice de corrélation deux à deux\n",
    "corr_matrix = my_data.drop(columns=['Points', 'Rank', 'Competition']).corr()\n",
    "\n",
    "# Initialisation figure\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "# Affichage heatmap\n",
    "sns.heatmap(corr_matrix, \n",
    "            vmin=-1, # borne inf des valeurs à afficher\n",
    "            vmax=1, # borne sup des valeurs à afficher\n",
    "            center= 0, # valeur médiane des valeurs à afficher,\n",
    "            cmap='PuOr', # colormap divergente de violet (PUrple) vers orange (ORange)\n",
    "           )\n",
    "# Titre\n",
    "plt.title(\"Corrélation entre les performances aux 10 épreuves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Quelles épreuves ont des performances très corrélées ? Quelles épreuves ont des performances indépendantes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__ \n",
    "* On retrouve la forte corrélation entre 100m et 110m haie (ainsi qu'avec le 400m), ou entre lancer de poids et lancer de disque.\n",
    "* Les performances aux épreuves de sprint (100m, 110m haie et 400m) sont anti-corrélées aux performances de saut en longueur. (Remarquez qu'une bonne performance à la course est une valeur _faible_, tandis qu'une bonne performance au saut est une valeur _élevée_.)\n",
    "* Les performances aux 110m haie sont indépendantes des performances au javelot, au saut à la perche ou au 1500m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ACP des scores aux 10 épreuves\n",
    "\n",
    "Nous allons maintenant effectuer une analyse en composantes principales des scores aux 10 épreuves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Création de l'array numpy correspondant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(my_data.drop(columns=['Points', 'Rank', 'Competition']))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ \n",
    "* Quel est le nombre d'individus dans la matrice de données `X` ?\n",
    "* Quel est le nombre de variables dans la matrice de données `X` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__ 41 individus, 10 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Standardisation des données\n",
    "\n",
    "Plus haut, nous avons standardisé les variables nous-mêmes en leur retirant leur moyenne et en les divisant par leur écart-type. Cette tâche est entièrement automatisée par `scikit-learn` :\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scale = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = std_scale.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Calcul des composantes principales\n",
    "\n",
    "Les algorithmes de factorisation de matrice de `scikit-learn` sont inclus dans le module `decomposition`. Pour  l'ACP, référez-vous à : \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque : Nous avons ici peu de variables et pouvons nous permettre de calculer toutes les PC. \n",
    "\n",
    "La plupart des algorithmes implémentés dans `scikit-learn` suivent le fonctionnement suivant : \n",
    "* on instancie un objet, correspondant à un type d'algorithme/modèle, avec ses hyperparamètres (ici le nombre de composantes)\n",
    "* on utilise la méthode `fit` pour passer les données à cet algorithme\n",
    "* les paramètres appris sont maintenant accessibles comme arguments de cet objet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation d'un objet PCA pour 10 composantes principales\n",
    "pca = decomposition.PCA(n_components=10)\n",
    "print(type(pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On passe maintenant les données standardisées à cet objet\n",
    "# C'est ici que se font les calculs\n",
    "pca.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Proportion de variance expliquée par les PCs\n",
    "\n",
    "Nous allons maintenant afficher la proportion de variance expliquée par les différentes composantes. Il est accessible dans le paramètre `explained_variance_ration_` de notre objet `pca`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, 11), pca.explained_variance_ratio_, marker='o')\n",
    "\n",
    "plt.xlabel(\"Nombre de composantes principales\")\n",
    "plt.ylabel(\"Proportion de variance expliquée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Affichez la proportion *cumulative* de variance expliquée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réponse\n",
    "\n",
    "np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.plot(np.arange(1, 11), np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
    "\n",
    "plt.xlabel(\"Nombre de composantes principales\")\n",
    "plt.title(\"Proportion cumulative de variance expliquée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Questions :__ \n",
    "* Quelle est la proportion de variance expliquée par les deux premières composantes ? \n",
    "* Combien de composantes faudrait-il utiliser pour expliquer 80% de la variance des données ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__\n",
    "* visuellement, 50% de la variance est expliquée par les 2 premières PC. On peut vérifier avec \n",
    "```python \n",
    "np.sum(pca.explained_variance_ratio_[:2])\n",
    "```\n",
    "ou\n",
    "```python\n",
    "np.cumsum(pca.explained_variance_ratio_)[1]\n",
    "```\n",
    "* 5 composantes (soit la moitié du nombre total de variables) permettent d'expliquer un peu plus que 80% de la variance des données. On peut vérifier avec\n",
    "```python \n",
    "np.sum(pca.explained_variance_ratio_[:5])\n",
    "```\n",
    "ou\n",
    "```python\n",
    "np.cumsum(pca.explained_variance_ratio_)[4]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Projection des données sur les deux premières composantes principales\n",
    "\n",
    "Nous allons maintenant utiliser uniquement les deux premières composantes principales.\n",
    "\n",
    "Commençons par calculer la nouvelle représentation des données, c'est-à-dire leur projection sur ces deux PC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(X_scaled)\n",
    "X_projected = pca.transform(X_scaled)\n",
    "print(X_projected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Affichez un nuage de points représentant les données selon ces deux PC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.scatter(X_projected[:, 0], X_projected[:, 1])\n",
    "\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Colorez chaque point du nuage de points ci-dessus en fonction du classement de l'athlète qu'il représente. Qu'en conclure sur l'interprétation de la PC1 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.scatter(X_projected[:, 0], X_projected[:, 1], c=my_data['Rank'])\n",
    "\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.colorbar(label='classement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les meilleurs athlètes sont plutôt d'un côté du graphe et les moins bons de l'autre. La première composante principale traduit la performance globale des athlètes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Interprétation des deux composantes principales\n",
    "Chaque composante principale est une combinaison linéaire des variables décrivant les données. Les poids de cette combinaison linéaire sont accesibles dans `pca.components_`.\n",
    "\n",
    "Nous pouvons visualiser non pas les individus, mais les 10 variables dans l'espace des 2 composantes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = pca.components_\n",
    "print(pcs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.scatter(pcs[0], pcs[1])\n",
    "for (x_coordinate, y_coordinate, feature_name) in zip(pcs[0], pcs[1], my_data.columns[:10]):\n",
    "    plt.text(x_coordinate, y_coordinate, feature_name)                          \n",
    "    \n",
    "plt.xlabel(\"Contribution à la PC1\")\n",
    "plt.ylabel(\"Contribution à la PC2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Quelles variables ont des contributions très similaires aux deux composantes principales ? Qu'en déduire sur leur similarité ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__ Les performances aux épreuves de 110m haie et de 100m, ou de lancer du poids et lancer du disque, sont très similaires. On imagine volontiers qu'elles requièrent les mêmes capacités physiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Comment interpréter le signe des contributions des variables à la première composantes principales ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__  Les variables contribuant négativement à la PC1 correspondent aux épreuves où un bon score correspond à une petite valeur (on mesure un temps, il faut être rapide) tandis que celles contribuant positivement correspondent aux épreuves où un bon score correspond à une valeur élevée (on mesure un temps, il faut aller loin).\n",
    "\n",
    "La première PC traduit ainsi la performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question facultative :__ Comment les valeurs de `pca.explained_variance_ratio_` sont-elles obtenues ? Refaites le calcul vous-mêmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Réponse possible\n",
    "tot_var = np.var(X_scaled, axis=0).sum()\n",
    "print((1 / tot_var) * np.var(X_projected, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Implémentation de l'ACP\n",
    "Cette partie s'inspire d'un notebook proposé par Joseph Boyd et Benoît Playe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Décomposition spectrale de la matrice de covariance\n",
    "\n",
    "Les deux premières composantes principales de X sont les deux vecteurs propres correspondant aux deux plus grandes valeurs propres de la matrice de covariance $\\Sigma = \\frac1n X^\\top X.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Utilisez `numpy.linalg` pour calculer les deux premières composantes principales de X. N'oubliez pas de travailler sur `X_scaled`. Comparez les à celles obtenues dans `pca.components_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg\n",
    "\n",
    "# Calcul de la matrice de covariance\n",
    "N = X_scaled.shape[0]\n",
    "X_cov = X_scaled.T.dot(X_scaled) / N\n",
    "\n",
    "# Décomposition spectrale\n",
    "eigenvals, eigenvecs = linalg.eig(X_cov)\n",
    "my_pcs = eigenvecs[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.scatter(my_pcs[:, 0], my_pcs[:, 1])\n",
    "for (x_coordinate, y_coordinate, feature_name) in zip(my_pcs[:, 0], my_pcs[:, 1], my_data.columns[:10]):\n",
    "    plt.text(x_coordinate, y_coordinate, feature_name)                          \n",
    "    \n",
    "plt.xlabel(\"Contribution à la PC1\")\n",
    "plt.ylabel(\"Contribution à la PC2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats correspondent, même s'il est possible d'obtenir ici des vecteurs opposés à ceux données par `sklearn` (ce qui n'est pas anormal : les PCs sont uniques au sens près). On peut le vérifier numériquement :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Calculez la nouvelle représentation en deux dimensions des données. Comparez à celle obtenue avec `pca.transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(my_pcs.T + pcs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_X = X_scaled.dot(my_pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.scatter(my_new_X[:, 0], my_new_X[:, 1], c=my_data['Rank'])\n",
    "                 #cmap=plt.get_cmap('viridis'))\n",
    "\n",
    "#rank_first_indices = np.flatnonzero(my_data['Rank']==1)\n",
    "#plt.scatter(X_projected[rank_first_indices, 0], X_projected[rank_first_indices, 1], label='Gagnants')\n",
    "\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.colorbar(label='classement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les axes peuvent être inversés par rapport à la figure obtenue avec scikit-learn, si les PCs sont opposées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Décomposition en valeurs singulières"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Utiliser `linalg.svd` pour retrouver les deux premières composantes principales. Comparer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = linalg.svd(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_pcs = V[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_pcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous retrouvons les mêmes PCs qu'avec `sklearn`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
